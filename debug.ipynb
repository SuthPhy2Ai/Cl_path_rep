{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2030625/89095818.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_path_feature.pth\"))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94713ff14bd4f74ab6fdf7797c12209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Features:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ase.db import connect\n",
    "from src import data_proc\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义 Schmidt 正交化函数（用于处理目标值）\n",
    "def schmidt_orthogonalization(vectors):\n",
    "    vectors = vectors.reshape(3, 3)\n",
    "    diagonal_elements = np.diagonal(vectors)\n",
    "    ele = np.mean(diagonal_elements)\n",
    "    return ele\n",
    "\n",
    "# 定义 MLP 模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 自定义数据集类\n",
    "class CrystalDataset(Dataset):\n",
    "    def __init__(self, db_path):\n",
    "        self.db = connect(db_path)\n",
    "        self.entries = list(self.db.select())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tmp = self.entries[idx]\n",
    "        atoms = tmp.toatoms()\n",
    "        try:\n",
    "            atom_feature = data_proc.get_crystal_path_muhead(ase_obj=True, stru=atoms, num_heads=1)\n",
    "            target = torch.tensor(tmp.data['dielectric'], dtype=torch.float32)  # 保留目标属性\n",
    "            atom_feature = torch.tensor(atom_feature, dtype=torch.float32)\n",
    "            return atom_feature, target  # 返回晶体特征和目标属性\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping structure {idx} due to error: {e}\")\n",
    "            return None\n",
    "\n",
    "# 自定义 collate 函数\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "\n",
    "    atom_features, targets = zip(*batch)\n",
    "    num_heads = atom_features[0].shape[0]\n",
    "    max_atoms = max(feat.shape[1] for feat in atom_features)\n",
    "    batch_size = len(atom_features)\n",
    "    embed_dim = atom_features[0].shape[2]\n",
    "\n",
    "    # 填充特征并展平 num_heads 维度\n",
    "    padded_features = torch.zeros((batch_size * num_heads, max_atoms, embed_dim), dtype=torch.float32)\n",
    "    attention_masks = torch.zeros((batch_size * num_heads, max_atoms), dtype=torch.float32)\n",
    "    flattened_targets = torch.zeros((batch_size * num_heads, *targets[0].shape), dtype=torch.float32)\n",
    "\n",
    "    for i, (feat, target) in enumerate(zip(atom_features, targets)):\n",
    "        num_atoms = feat.shape[1]\n",
    "        for head in range(num_heads):\n",
    "            idx = i * num_heads + head\n",
    "            padded_features[idx, :num_atoms, :] = feat[head]\n",
    "            attention_masks[idx, :num_atoms] = 1\n",
    "            flattened_targets[idx] = target\n",
    "\n",
    "    return padded_features, attention_masks, flattened_targets\n",
    "\n",
    "# 定义 GRU 编码器\n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h_n = self.gru(x)  # h_n: (num_layers, batch_size, hidden_dim)\n",
    "        h_n = h_n[-1]  # 取最后一层的隐藏状态\n",
    "        output = self.fc(h_n)\n",
    "        return output\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim, output_dim):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.encoder = GRUEncoder(feature_dim, hidden_dim, output_dim)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is None:\n",
    "            return self.encoder(x1)\n",
    "        z1 = self.encoder(x1)\n",
    "        z2 = self.encoder(x2)\n",
    "        return z1, z2\n",
    "# 加载预训练模型\n",
    "feature_dim = 384\n",
    "hidden_dim = 256\n",
    "output_dim = 128\n",
    "model = ContrastiveModel(feature_dim, hidden_dim, output_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"best_path_feature.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 加载数据集\n",
    "dataset = CrystalDataset(\"/data/home/hzw1010/suth/elec_gw/dbs/clean.db\")\n",
    "dataloader = DataLoader(dataset, batch_size=64, num_workers=2, pin_memory=False,\n",
    "                        shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "# 提取特征\n",
    "def extract_features(model, dataloader, device):\n",
    "    features = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_masks, batch_targets in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "            if batch_features is None:\n",
    "                continue\n",
    "            batch_features = batch_features.to(device)\n",
    "            z = model(batch_features)  # 提取特征\n",
    "            features.append(z.cpu())\n",
    "            targets.append(batch_targets.cpu())\n",
    "    return torch.cat(features, dim=0), torch.cat(targets, dim=0)\n",
    "\n",
    "# 处理目标值\n",
    "def process_targets(targets):\n",
    "    processed_targets = []\n",
    "    for target in targets:\n",
    "        elec = schmidt_orthogonalization(target.numpy())\n",
    "        processed_targets.append(elec)\n",
    "    return torch.tensor(processed_targets, dtype=torch.float32)\n",
    "\n",
    "# 提取特征并处理目标值\n",
    "features, targets = extract_features(model, dataloader, device)\n",
    "processed_targets = process_targets(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250116_060244\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "dataset_size = len(features)\n",
    "train_size = int(0.8 * dataset_size)  \n",
    "train_data = pd.DataFrame(features[:train_size].numpy())\n",
    "train_labels = pd.Series(processed_targets[:train_size].numpy())\n",
    "val_data = pd.DataFrame(features[train_size:].numpy())\n",
    "val_labels = pd.Series(processed_targets[train_size:].numpy())\n",
    "predictor = TabularPredictor(label=\"target\", eval_metric=\"mean_squared_error\")\n",
    "train_data['target'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Tue Jun 1 16:14:33 UTC 2021\n",
      "CPU Count:          96\n",
      "Memory Avail:       234.09 GB / 251.30 GB (93.2%)\n",
      "Disk Space Avail:   529624.39 GB / 596060.00 GB (88.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 9000s of the 36000s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-01-16 14:03:18,429\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/data/home/hzw1010/suth/elec_gw/AutogluonModels/ag-20250116_060244/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Beginning AutoGluon training ... Time limit = 8983s\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m AutoGluon will save models to \"/data/home/hzw1010/suth/elec_gw/AutogluonModels/ag-20250116_060244/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Train Data Rows:    5148\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Train Data Columns: 128\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Label Column:       target\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tAvailable Memory:                    237381.16 MB\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tTrain Data (Original)  Memory Usage: 2.51 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t\t('float', []) : 128 | ['0', '1', '2', '3', '4', ...]\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t\t('float', []) : 128 | ['0', '1', '2', '3', '4', ...]\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t128 features in original data used to generate 128 features in processed data.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tTrain Data (Processed) Memory Usage: 2.51 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Data preprocessing and feature engineering runtime = 0.24s ...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5987.06s of the 8982.83s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.9884\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.38s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5986.59s of the 8982.36s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.9335\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.24s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5986.27s of the 8982.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.02%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2100138)\u001b[0m [1000]\tvalid_set's l2: 21.1798\n",
      "\u001b[36m(_ray_fit pid=2100138)\u001b[0m [2000]\tvalid_set's l2: 21.1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-23.6093\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t5.86s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 5959.26s of the 8955.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.02%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.0074\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t3.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 5935.10s of the 8930.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-27.1788\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t5.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.31s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 5929.59s of the 8925.36s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.8862\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t68.53s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 5840.57s of the 8836.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.1648\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.73s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.32s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 5839.18s of the 8834.95s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-22.4997\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t33.72s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 5781.22s of the 8776.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.4612\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t7.35s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5744.26s of the 8740.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-19.0358\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t139.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5581.93s of the 8577.70s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.05%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2131446)\u001b[0m [1000]\tvalid_set's l2: 27.2697\n",
      "\u001b[36m(_ray_fit pid=2131453)\u001b[0m [1000]\tvalid_set's l2: 24.2149\n",
      "\u001b[36m(_ray_fit pid=2131446)\u001b[0m [2000]\tvalid_set's l2: 27.2692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-27.3011\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t22.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 5526.50s of the 8522.27s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.8064\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t71.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 5429.52s of the 8425.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-19.22\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t89.91s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 5311.27s of the 8307.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.02%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2140729)\u001b[0m [1000]\tvalid_set's l2: 22.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.8941\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t9.07s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 5266.45s of the 8262.22s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_ray_fit pid=2142435)\u001b[0m No improvement since epoch 20: early stopping\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-22.0727\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t43.92s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 5193.79s of the 8189.56s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.079\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t347.88s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 4807.36s of the 7803.13s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=2142436)\u001b[0m No improvement since epoch 22: early stopping\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2177008)\u001b[0m [1000]\tvalid_set's l2: 25.88\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2177009)\u001b[0m [7000]\tvalid_set's l2: 21.0293\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-23.6188\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t11.65s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 4767.25s of the 7763.02s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-18.059\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t141.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 4599.19s of the 7594.96s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.17%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.4446\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t39.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 4518.22s of the 7513.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.0331\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.68s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.31s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 4516.99s of the 7512.76s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.0706\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t57.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 4433.94s of the 7429.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-23.6116\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t40.41s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 4357.27s of the 7353.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.4946\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t152.52s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 4168.36s of the 7164.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-27.0282\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t3.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.34s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 4164.06s of the 7159.83s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.05%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2197181)\u001b[0m [1000]\tvalid_set's l2: 25.8943\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-23.8715\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t13.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 4119.46s of the 7115.23s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-21.6528\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t47.31s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 4041.65s of the 7037.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.664\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t9.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 3995.66s of the 6991.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-20.3473\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t88.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 3874.73s of the 6870.50s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.04%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2276365)\u001b[0m [1000]\tvalid_set's l2: 22.3275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.9069\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t7.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 3827.56s of the 6823.33s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.6908\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t85.94s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 3714.21s of the 6709.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.6271\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t82.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 3593.32s of the 6589.09s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.5798\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t36.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 3522.57s of the 6518.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.05%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.5205\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t8.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 3488.85s of the 6484.62s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.1012\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.47s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.23s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 3487.91s of the 6483.68s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.02%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.4391\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t58.43s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 3410.62s of the 6406.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_ray_fit pid=2363045)\u001b[0m No improvement since epoch 16: early stopping\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-21.9508\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t31.63s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 3355.00s of the 6350.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-20.0475\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t89.94s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 3238.03s of the 6233.80s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.09%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2393376)\u001b[0m [1000]\tvalid_set's l2: 24.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.8513\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t15.93s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 3195.71s of the 6191.48s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_ray_fit pid=2394984)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.881\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t27.23s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 3149.62s of the 6145.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.2782\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t154.52s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 2960.63s of the 5956.40s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_ray_fit pid=2402661)\u001b[0m No improvement since epoch 23: early stopping\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.154\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t37.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 2891.27s of the 5887.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.05%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2405410)\u001b[0m [1000]\tvalid_set's l2: 15.078\n",
      "\u001b[36m(_ray_fit pid=2405407)\u001b[0m [1000]\tvalid_set's l2: 21.6014\n",
      "\u001b[36m(_ray_fit pid=2405407)\u001b[0m [2000]\tvalid_set's l2: 20.928\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2405411)\u001b[0m [3000]\tvalid_set's l2: 34.1131\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2405405)\u001b[0m [4000]\tvalid_set's l2: 25.4984\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2405405)\u001b[0m [6000]\tvalid_set's l2: 25.4811\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2405405)\u001b[0m [7000]\tvalid_set's l2: 25.4762\n",
      "\u001b[36m(_ray_fit pid=2405405)\u001b[0m [8000]\tvalid_set's l2: 25.474\n",
      "\u001b[36m(_ray_fit pid=2405405)\u001b[0m [9000]\tvalid_set's l2: 25.4738\n",
      "\u001b[36m(_ray_fit pid=2405405)\u001b[0m [10000]\tvalid_set's l2: 25.4729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-23.5651\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t56.28s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.29s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 2794.10s of the 5789.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.2651\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t3.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.27s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 2790.57s of the 5786.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.05%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.7583\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t86.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r95_BAG_L1 ... Training model for up to 2675.79s of the 5671.56s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-21.9147\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t47.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r41_BAG_L1 ... Training model for up to 2591.34s of the 5587.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-19.9285\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t172.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: XGBoost_r98_BAG_L1 ... Training model for up to 2386.17s of the 5381.94s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.9265\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t83.95s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r15_BAG_L1 ... Training model for up to 2269.66s of the 5265.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2494537)\u001b[0m [1000]\tvalid_set's l2: 16.6906\n",
      "\u001b[36m(_ray_fit pid=2494539)\u001b[0m [5000]\tvalid_set's l2: 22.168\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2494536)\u001b[0m [10000]\tvalid_set's l2: 27.4734\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.6312\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t15.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r158_BAG_L1 ... Training model for up to 2224.59s of the 5220.36s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-28.6687\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t96.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r86_BAG_L1 ... Training model for up to 2102.00s of the 5097.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.5858\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t130.65s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r37_BAG_L1 ... Training model for up to 1932.00s of the 4927.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_ray_fit pid=2504585)\u001b[0m No improvement since epoch 19: early stopping\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-22.4057\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t42.27s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r197_BAG_L1 ... Training model for up to 1855.11s of the 4850.88s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.2415\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t70.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r49_BAG_L1 ... Training model for up to 1747.13s of the 4742.90s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.0993\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t58.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: ExtraTrees_r49_BAG_L1 ... Training model for up to 1647.41s of the 4643.17s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.4464\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.29s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r143_BAG_L1 ... Training model for up to 1646.07s of the 4641.84s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.06%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2514786)\u001b[0m [1000]\tvalid_set's l2: 22.4115\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.0164\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t17.93s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: RandomForest_r127_BAG_L1 ... Training model for up to 1596.08s of the 4591.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.5047\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t3.18s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.24s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r134_BAG_L1 ... Training model for up to 1592.42s of the 4588.19s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-22.8532\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t38.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: RandomForest_r34_BAG_L1 ... Training model for up to 1529.27s of the 4525.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-29.2758\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t1.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r94_BAG_L1 ... Training model for up to 1527.24s of the 4523.00s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2519324)\u001b[0m [1000]\tvalid_set's l2: 27.263\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-23.5106\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t7.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r143_BAG_L1 ... Training model for up to 1485.20s of the 4480.97s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-18.9871\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t197.8s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r128_BAG_L1 ... Training model for up to 1251.07s of the 4246.84s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-24.7243\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t295.48s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r111_BAG_L1 ... Training model for up to 916.52s of the 3912.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.8352\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t39.23s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r31_BAG_L1 ... Training model for up to 846.18s of the 3841.95s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-26.0936\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t68.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: ExtraTrees_r4_BAG_L1 ... Training model for up to 743.23s of the 3739.00s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-28.6197\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.51s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r65_BAG_L1 ... Training model for up to 742.28s of the 3738.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.7003\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t39.87s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetFastAI_r88_BAG_L1 ... Training model for up to 662.74s of the 3658.51s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_ray_fit pid=2542908)\u001b[0m No improvement since epoch 28: early stopping\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-23.7711\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t36.76s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: LightGBM_r30_BAG_L1 ... Training model for up to 590.60s of the 3586.36s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.04%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2545618)\u001b[0m [1000]\tvalid_set's l2: 20.5562\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2545619)\u001b[0m [2000]\tvalid_set's l2: 21.2223\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-23.4622\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t17.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: XGBoost_r49_BAG_L1 ... Training model for up to 539.37s of the 3535.14s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.05%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.5289\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t15.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r5_BAG_L1 ... Training model for up to 494.21s of the 3489.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-25.4059\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t57.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r87_BAG_L1 ... Training model for up to 406.34s of the 3402.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_quality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m36000\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1280\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_stacking:\n\u001b[1;32m   1275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDyStack is enabled (dynamic_stacking=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdynamic_stacking\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     num_stack_levels, time_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_stacking\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mds_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting main fit with num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mFor future fit calls on this dataset, you can skip DyStack to save time: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`predictor.fit(..., dynamic_stacking=False, num_stack_levels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_stack_levels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1285\u001b[0m     )\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (time_limit \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1380\u001b[0m, in \u001b[0;36mTabularPredictor._dynamic_stacking\u001b[0;34m(self, ag_fit_kwargs, ag_post_fit_kwargs, validation_procedure, detection_time_frac, holdout_frac, n_folds, n_repeats, memory_safe_fits, clean_up_fits, enable_ray_logging, enable_callbacks, holdout_data)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         _, holdout_data, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_data(train_data\u001b[38;5;241m=\u001b[39mX, tuning_data\u001b[38;5;241m=\u001b[39mholdout_data)\n\u001b[1;32m   1378\u001b[0m         ds_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds_fit_context\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ds_fit_context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_fit_custom_ho\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1380\u001b[0m     stacked_overfitting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sub_fit_memory_save_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_ag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_ag_post_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mholdout_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mholdout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;66;03m# Holdout is false, use (repeated) cross-validation\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m     is_stratified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;129;01min\u001b[39;00m [REGRESSION, QUANTILE, SOFTCLASS]\n",
      "File \u001b[0;32m~/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1563\u001b[0m, in \u001b[0;36mTabularPredictor._sub_fit_memory_save_wrapper\u001b[0;34m(self, train_data, time_limit, time_start, ds_fit_kwargs, ag_fit_kwargs, ag_post_fit_kwargs, holdout_data)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;66;03m# FIXME: For some reason ray does not treat `num_cpus` and `num_gpus` the same.\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m#  For `num_gpus`, the process will reserve the capacity and is unable to share it to child ray processes, causing a deadlock.\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;66;03m#  For `num_cpus`, the value is completely ignored by children, and they can even use more num_cpus than the parent.\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m#  Because of this, num_gpus is set to 0 here to avoid a deadlock, but num_cpus does not need to be changed.\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;66;03m#  For more info, refer to Ray documentation: https://docs.ray.io/en/latest/ray-core/tasks/nested-tasks.html#yielding-resources-while-blocked\u001b[39;00m\n\u001b[1;32m   1554\u001b[0m ref \u001b[38;5;241m=\u001b[39m sub_fit_caller\u001b[38;5;241m.\u001b[39moptions(num_cpus\u001b[38;5;241m=\u001b[39mnum_cpus, num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mremote(\n\u001b[1;32m   1555\u001b[0m     predictor\u001b[38;5;241m=\u001b[39mpredictor_ref,\n\u001b[1;32m   1556\u001b[0m     train_data\u001b[38;5;241m=\u001b[39mtrain_data_ref,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     holdout_data\u001b[38;5;241m=\u001b[39mholdout_data_ref,\n\u001b[1;32m   1562\u001b[0m )\n\u001b[0;32m-> 1563\u001b[0m finished, unfinished \u001b[38;5;241m=\u001b[39m \u001b[43m_ds_ray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mref\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m stacked_overfitting, ho_leaderboard, exception \u001b[38;5;241m=\u001b[39m _ds_ray\u001b[38;5;241m.\u001b[39mget(finished[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1566\u001b[0m \u001b[38;5;66;03m# TODO: This is present to ensure worker logs are properly logged and don't get skipped / printed out of order.\u001b[39;00m\n\u001b[1;32m   1567\u001b[0m \u001b[38;5;66;03m#  Ideally find a faster way to do this that doesn't introduce a 100 ms overhead.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/elec/lib/python3.9/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/elec/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/elec/lib/python3.9/site-packages/ray/_private/worker.py:2984\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2982\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2983\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2984\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3816\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/includes/common.pxi:79\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-21.1266\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t173.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: NeuralNetTorch_r71_BAG_L1 ... Training model for up to 194.05s of the 3189.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.01%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t-21.1389\t = Validation score   (-mean_squared_error)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t77.52s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Fitting model: CatBoost_r143_BAG_L1 ... Training model for up to 85.09s of the 3080.86s of remaining time.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=12, gpus=0, memory=0.05%)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \tWarning: Exception caused CatBoost_r143_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=2561284, ip=10.100.100.132)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self.model.fit(X, **fit_final_kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/catboost/core.py\", line 5873, in fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/catboost/core.py\", line 2410, in _fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self._train(\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/catboost/core.py\", line 1790, in _train\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m ray.exceptions.TaskCancelledError: Task: TaskID(7161ae05b5f66b8cffffffffffffffffffffffff01000000) was cancelled.\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m ray.exceptions.RayTaskError(TaskCancelledError): \u001b[36mray::_ray_fit()\u001b[39m (pid=2561284, ip=10.100.100.132)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 243, in _fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self.model.fit(X, **fit_final_kwargs)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/catboost/core.py\", line 5873, in fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/catboost/core.py\", line 2410, in _fit\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self._train(\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"/data/home/hzw1010/anaconda3/envs/elec/lib/python3.9/site-packages/catboost/core.py\", line 1790, in _train\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m     self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m   File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "\u001b[36m(_dystack pid=2088280)\u001b[0m ray.exceptions.TaskCancelledError: Task: TaskID(7161ae05b5f66b8cffffffffffffffffffffffff01000000) was cancelled.\n"
     ]
    }
   ],
   "source": [
    "predictor.fit(\n",
    "    train_data, \n",
    "    presets=\"best_quality\",\n",
    "    time_limit=36000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x7ff448f70d00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_absolute_error': -1.754349708557129, 'root_mean_squared_error': -5.314374923706055, 'mean_squared_error': -28.242578506469727, 'r2': 0.5191915035247803, 'pearsonr': 0.7241694160215834, 'median_absolute_error': -0.5566833019256592}\n"
     ]
    }
   ],
   "source": [
    "# 评估模型性能\n",
    "val_data['target'] = val_labels\n",
    "performance = predictor.evaluate(val_data)\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_input_dim = output_dim  # 输入维度是特征维度\n",
    "mlp_hidden_dim = 128\n",
    "mlp_output_dim = 1  # 输出是一个标量\n",
    "mlp_model = MLP(mlp_input_dim, mlp_hidden_dim, mlp_output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(features)\n",
    "train_size = int(0.8 * dataset_size)  # 80% 训练集\n",
    "val_size = dataset_size - train_size  # 20% 验证集\n",
    "train_dataset = TensorDataset(features[:train_size], processed_targets[:train_size])\n",
    "train_dataset = TensorDataset(features, processed_targets)\n",
    "val_dataset = TensorDataset(features[train_size:], processed_targets[train_size:])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "mlp_input_dim = output_dim  # 输入维度是特征维度\n",
    "mlp_hidden_dim = 128\n",
    "mlp_output_dim = 1  # 输出是一个标量\n",
    "mlp_model = MLP(mlp_input_dim, mlp_hidden_dim, mlp_output_dim).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-4)\n",
    "num_epochs = 2000\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import logging\n",
    "\n",
    "# 配置日志记录\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mlp_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # 训练阶段\n",
    "    with tqdm(train_dataloader, desc=f\"Epoch {epoch+1} - Training\", unit=\"batch\") as t:\n",
    "        for batch_features, batch_targets in t:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = mlp_model(batch_features)\n",
    "            loss = criterion(predictions.squeeze(), batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            t.set_postfix({'Batch Train Loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # 验证阶段\n",
    "    mlp_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with tqdm(val_dataloader, desc=f\"Epoch {epoch+1} - Validation\", unit=\"batch\") as v:\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in v:\n",
    "                batch_features = batch_features.to(device)\n",
    "                batch_targets = batch_targets.to(device)\n",
    "                predictions = mlp_model(batch_features)\n",
    "                loss = criterion(predictions.squeeze(), batch_targets)\n",
    "                val_loss += loss.item()\n",
    "                v.set_postfix({'Batch Val Loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # 计算平均损失\n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    # 更新训练和验证损失到进度条的描述信息\n",
    "    #tqdm.write(f\"Epoch {epoch+1} - Final Train Loss: {train_loss:.4f}, Final Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # 保存最佳模型（记录到日志中）\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(mlp_model.state_dict(), \"best_mlp_model.pth\")\n",
    "        logging.info(f\"Best MLP model saved with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # 在下一个 epoch 开始前，更新上一个 epoch 的最终损失信息到进度条中\n",
    "    if epoch < num_epochs - 1:\n",
    "        tqdm.write(\"\")  # 空行分隔不同的 epoch 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            predictions = model(batch_features).squeeze()\n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_targets.append(batch_targets.cpu())\n",
    "\n",
    "    # 合并所有批次数据\n",
    "    all_predictions = torch.cat(all_predictions).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    # 计算 MAE 和 R²\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "    return all_targets, all_predictions, mae, r2\n",
    "def plot_scatter(targets, predictions):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(targets, predictions, alpha=0.6, label=\"Predictions vs Targets\")\n",
    "    plt.plot([min(targets), max(targets)], [min(targets), max(targets)], color=\"red\", linestyle=\"--\", label=\"Ideal\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f'Scatter Plot of True vs Predicted Values\\nMAE: {mae:.4f}, R²: {r2:.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "# 加载最佳模型\n",
    "mlp_model.load_state_dict(torch.load(\"best_mlp_model.pth\"))\n",
    "\n",
    "# 测试模型性能并绘图\n",
    "test_targets, test_predictions, mae, r2 = evaluate_model(mlp_model, val_dataloader, device)\n",
    "print(f\"MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# 画散点图\n",
    "plot_scatter(test_targets, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
