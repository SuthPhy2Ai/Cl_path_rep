{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from ase.io import read\n",
    "from ase.db import connect\n",
    "import spglib\n",
    "from gensim.models import Word2Vec\n",
    "from mendeleev import element\n",
    "\n",
    "# Constants\n",
    "NUM_SLICE = 10\n",
    "# Get the absolute path of the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "SRC_DIR = os.path.join(parent_dir, \"src\")\n",
    "# Load necessary datasets\n",
    "space_group_df = pd.read_csv(os.path.join(SRC_DIR, \"point_group_array.csv\"))\n",
    "w2v_model = Word2Vec.load(os.path.join(SRC_DIR, \"mat2vec-master/mat2vec/training/models/pretrained_embeddings\"))\n",
    "atoms_vec = json.load(open(os.path.join(SRC_DIR, \"atom_init.json\"), \"r\"))\n",
    "element_data_df = pd.read_json(os.path.join(SRC_DIR, \"allrs.json\"), orient='index')\n",
    "\n",
    "def get_value_by_element(df, element_symbol):\n",
    "    \"\"\"Retrieve a value from the dataframe based on the element symbol.\"\"\"\n",
    "    if element_symbol in df[0].values:\n",
    "        return df[df[0] == element_symbol].iloc[0, 1]\n",
    "    return None\n",
    "\n",
    "def tsp_dijkstra(data, start_node):\n",
    "    \"\"\"Solve the Traveling Salesman Problem (TSP) using a greedy heuristic.\"\"\"\n",
    "    n = data.shape[0]\n",
    "    graph = np.full((n, n), np.inf)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                graph[i, j] = np.linalg.norm(data[i] - data[j])\n",
    "    path = {start_node}\n",
    "    curr_index = start_node\n",
    "    distances, predecessors = [], []\n",
    "    while len(path) < n:\n",
    "        min_distance, min_index = np.inf, -1\n",
    "        for i in range(n):\n",
    "            if i not in path and graph[curr_index, i] < min_distance:\n",
    "                min_distance = graph[curr_index, i]\n",
    "                min_index = i\n",
    "        if min_index != -1:\n",
    "            path.add(min_index)\n",
    "            curr_index = min_index\n",
    "            distances.append(min_distance)\n",
    "            predecessors.append(curr_index)\n",
    "    total_distance = sum(distances)\n",
    "    path_nodes = [start_node] + list(predecessors)\n",
    "    return total_distance, path_nodes\n",
    "\n",
    "def build_dist_matrix(data):\n",
    "    \"\"\"Construct a distance matrix based on Euclidean distances.\"\"\"\n",
    "    n = data.shape[0]\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dist = np.linalg.norm(data[i] - data[j])\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = dist\n",
    "    return dist_matrix\n",
    "\n",
    "def get_embed_dist(dist_matrix, lst, win):\n",
    "    \"\"\"Compute node distances using a sliding window approach.\"\"\"\n",
    "    dist_lst = []\n",
    "    for idx in range(len(lst)):\n",
    "        if idx < len(lst) - win:\n",
    "            tmp_lst = [lst[idx], lst[idx + win]]\n",
    "        else:\n",
    "            tmp_lst = [lst[idx], lst[idx - win]]\n",
    "        dist = dist_matrix[tmp_lst[0], tmp_lst[1]]\n",
    "        dist_lst.append(min(1 / (dist**2), 1))\n",
    "    return dist_lst\n",
    "\n",
    "def get_embed_mat2v_global(formula=\"BaTiO3\"):\n",
    "    \"\"\"Retrieve the element embedding vector using the mat2vec method.\"\"\"\n",
    "    return w2v_model.wv.get_mean_vector(formula)\n",
    "\n",
    "def gaussian_basis_functions(d):\n",
    "    \"\"\"Expand distances using Gaussian basis functions.\"\"\"\n",
    "    mu_values = np.linspace(0.2, 1.7, 24)\n",
    "    gamma = 10\n",
    "    return np.exp(-gamma * (d - mu_values) ** 2)\n",
    "\n",
    "def get_crystal_path_muhead(ase_obj=True, \n",
    "                            stru=None, \n",
    "                            ids=None, \n",
    "                            all_files=None, \n",
    "                            num=None, \n",
    "                            work_path=None, \n",
    "                            mpid=None, \n",
    "                            elec=None, \n",
    "                            nebmax=4, \n",
    "                            num_heads=4):\n",
    "    \"\"\"Process the structure and compute features for randomly selected atomic heads.\"\"\"\n",
    "    if not ase_obj:\n",
    "        cif_path = all_files[num]\n",
    "        ids = all_files[num].split('/')[-1].split('.')[0]\n",
    "        stru = read(cif_path)\n",
    "    # \n",
    "    # if len(stru) > 30 or len(stru) < 4:\n",
    "    #     raise ValueError(f\"Invalid number of atoms: {len(stru)}\")\n",
    "    if len(stru) < 4:\n",
    "        stru = stru * (2, 2, 2) # duplicate the unit cell to make sure there are enough atoms\n",
    "    if len(stru) > 30:\n",
    "        #raise ValueError(f\"TOO MANY number of atoms in unit cell: {len(stru)}\")\n",
    "        pass\n",
    "    # print(\"there is {} atoms in the unit cell\".format(len(stru)))\n",
    "    selected_heads = random.sample(range(len(stru)), min(num_heads, len(stru)))\n",
    "    Atom_feature = []\n",
    "    for head in selected_heads:\n",
    "        atom_feature = []\n",
    "        elem_list = stru.get_chemical_symbols()\n",
    "        node_list = stru.get_atomic_numbers()\n",
    "        point_set = stru.get_positions()\n",
    "        formula = stru.get_chemical_formula()\n",
    "        data = point_set.copy()\n",
    "        distance, path = tsp_dijkstra(data, head)\n",
    "        dist_matrix = build_dist_matrix(data)\n",
    "        rs__ = np.zeros((24, len(path)))\n",
    "        for m in [elem_list[i] for i in path]:\n",
    "            rs = get_value_by_element(element_data_df, m)\n",
    "            rs__[:, list([elem_list[i] for i in path]).index(m)] = gaussian_basis_functions(rs)\n",
    "        dist__ = np.zeros((nebmax, len(path)))\n",
    "        for i in range(1, nebmax + 1):\n",
    "            dist__[i-1, :] = get_embed_dist(dist_matrix, path, win=i)\n",
    "        mat2v_global = get_embed_mat2v_global(formula)\n",
    "        spacegroup_info = spglib.get_symmetry_dataset(stru)\n",
    "        # print(\"spacegroup_info\",spacegroup_info)\n",
    "        \n",
    "        pg = spacegroup_info[\"pointgroup\"]\n",
    "        sym2v_global = np.array(space_group_df[space_group_df[\"point_group\"] == pg])[0][1:].astype(int)\n",
    "        stru.info.update({\"rs__\": rs__, \n",
    "                          \"mat2v_global\": mat2v_global, \n",
    "                          \"dist__\": dist__, \n",
    "                          \"path_indx\": node_list[path], \n",
    "                          \"sym2v_global\": sym2v_global})\n",
    "        for i, k in enumerate(list(node_list[path])):\n",
    "            tmp_feature = np.concatenate([atoms_vec[str(k)], mat2v_global, sym2v_global, dist__[:, i], rs__[:, i]])\n",
    "            atom_feature.append(tmp_feature)\n",
    "        # feat_db = connect(f\"{work_path}/deal_json.db\")\n",
    "        # feat_db.write(stru, ids=ids, head=head, atom_feature=json.dumps(atom_feature.tolist()), mpid=mpid, data={'atom_feature': atom_feature, 'elec': np.array(elec)})\n",
    "        Atom_feature.append(atom_feature)\n",
    "    return np.array(Atom_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 114/114 [01:46<00:00,  1.07batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 106.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ase.db import connect\n",
    "import numpy as np\n",
    "\n",
    "class CrystalDataset(Dataset):\n",
    "    def __init__(self, db_path):\n",
    "        self.db = connect(db_path)\n",
    "        self.entries = list(self.db.select())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tmp = self.entries[idx]\n",
    "        atoms = tmp.toatoms()\n",
    "        try:\n",
    "            atom_feature = get_crystal_path_muhead(ase_obj=True, stru=atoms)\n",
    "            target = torch.tensor(tmp.data['dielectric'], dtype=torch.float32)\n",
    "            atom_feature = torch.tensor(atom_feature, dtype=torch.float32)\n",
    "            return atom_feature, target\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping structure {idx} due to error: {e}\")\n",
    "            return None \n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that handles skipping None samples.\n",
    "    \"\"\"\n",
    "    batch = [b for b in batch if b is not None] \n",
    "    if len(batch) == 0:\n",
    "        return None \n",
    "    atom_features, targets = zip(*batch)\n",
    "    max_atoms = max(feat.shape[1] for feat in atom_features)\n",
    "    batch_size = len(atom_features)\n",
    "    num_heads, embed_dim = atom_features[0].shape[0], atom_features[0].shape[2]\n",
    "\n",
    "    padded_features = torch.zeros((batch_size, num_heads, max_atoms, embed_dim), dtype=torch.float32)\n",
    "    attention_masks = torch.zeros((batch_size, num_heads, max_atoms), dtype=torch.float32)\n",
    "    for i, feat in enumerate(atom_features):\n",
    "        num_atoms = feat.shape[1]\n",
    "        padded_features[i, :, :num_atoms, :] = feat\n",
    "        attention_masks[i, :, :num_atoms] = 1 \n",
    "    targets = torch.stack(targets)\n",
    "    return padded_features, attention_masks, targets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = CrystalDataset(\"/data/home/hzw1010/suth/elec_gw/dbs/dielectric_with_gap.db\")\n",
    "dataloader = DataLoader(dataset, batch_size=64,  num_workers=2,pin_memory=False,\n",
    "                        shuffle=True, collate_fn=collate_fn, drop_last=False)\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import time\n",
    "start_time = time.time()\n",
    "for batch_features, batch_masks, batch_targets in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "    pass  \n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
